{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fdc299f",
   "metadata": {},
   "source": [
    "### cGANs in Astronomy\n",
    "Normalising the data properly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c64363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import time\n",
    "from astropy.io import fits\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\\..*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88fedff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to working directory (use directory where your data is saved)\n",
    "home = \"/Users/ruby/Documents/Python Scripts/Filters/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0715e9",
   "metadata": {},
   "source": [
    "Before we create our dataset and dataloaders, both the train and test data need to be normalised for the model to train. The data are normalised on the interval [0,1] using the below function. You may wish to change this using more efficient methods such as MinMaxScaler() from the sklearn.preprocessing package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d86b007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the values between 0 and 1\n",
    "def Normalise(data, lower=0, upper=1):\n",
    "    return ((data - data.min())/ (data.max() - data.min()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944a2a57",
   "metadata": {},
   "source": [
    "Dataset and Dataloaders\n",
    "\n",
    "Here we create the dataset for our model. First, the data must be transformed to the correct size for the model (256x256). The data in each waveband file are read, appended as an input for short wavebands (as a label for long wavebands) and normalised using the above function. Both the inputs and labels are transformed to a tensor of shape [(channel, height, width)] before being resized to 256x256. We then split the dataset into training and testing, using 90% of the data to train and 10% for testing, before creating the dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5babfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "SIZE = 256    \n",
    "\n",
    "# create the dataset for the fits files\n",
    "class FilterDataset(Dataset):\n",
    "    def __init__(self, path):\n",
    "        ''' path = path to directory containing fits files '''\n",
    "        self.path = path\n",
    "        self.transforms_inputs = torch.nn.Sequential(transforms.Resize((SIZE, SIZE), antialias=True))\n",
    "                                                     \n",
    "        self.transforms_labels = torch.nn.Sequential(transforms.Resize((SIZE, SIZE), antialias=True))\n",
    "        self.data = []\n",
    "        self.f115w_path = path+'F115W/'\n",
    "        self.l1 = len(os.listdir(self.f115w_path)) - 1 # might want to remove -1\n",
    "    \n",
    "    def __len__(self):\n",
    "        # return total number of fits files for the galaxy cutouts consistent\n",
    "        # with the 'idx' in the __getitem__ method\n",
    "        return (self.l1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # get the name of the fits file\n",
    "        name = str(idx)+'.fits'\n",
    "        # for each input filter, get each fits file, open and extract the \n",
    "        # first row (only row which is 'SCI' data) and normalise before\n",
    "        # formatting into an array\n",
    "        for i in range(nbands):\n",
    "            root = self.path+filters[i]\n",
    "            hdu = fits.open(root+name)[0]\n",
    "            data_img = hdu.data\n",
    "            self.data.append(data_img)\n",
    "        data_array = np.array(self.data)\n",
    "        data_norm = Normalise(data_array)\n",
    "        data_norm_Nbands2 = np.split(data_norm, nbands)\n",
    "        f115w = data_norm_Nbands2[0]\n",
    "        f150w = data_norm_Nbands2[1]\n",
    "        f200w = data_norm_Nbands2[2]\n",
    "        f277w = data_norm_Nbands2[3]\n",
    "        f356w = data_norm_Nbands2[4]\n",
    "        f444w = data_norm_Nbands2[5]\n",
    "        \n",
    "        # stack the input filters (f115w, f150w, f200w)\n",
    "        inputs = np.dstack((f115w, f150w, f200w)).astype(\"float32\")\n",
    "        # reformat the inputs as tensors\n",
    "        inputs = transforms.ToTensor()(inputs)\n",
    "        # reshape the tensor to [C, H, W] for the transform to work\n",
    "        inputs = inputs.permute(0, 1, 2)\n",
    "        # now resize the inputs to 256x256\n",
    "        inputs = self.transforms_inputs(inputs)\n",
    "\n",
    "        # do the same for the labels\n",
    "        labels = np.dstack((f277w, f356w, f444w)).astype(\"float32\")\n",
    "        labels = transforms.ToTensor()(labels)\n",
    "        labels = labels.permute(0, 1, 2)\n",
    "        labels = self.transforms_labels(labels)\n",
    "\n",
    "        # return the inputs with corresponding labels in a dictionary\n",
    "        return {'Inputs': inputs, 'Labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093763cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FilterDataset(path=home)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67556687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the generated dataset into training and testing \n",
    "BATCH_SIZE = 16                                     # set the batch size\n",
    "VALIDATION_SPLIT = 0.1                              # set the validation split of 10%\n",
    "SHUFFLE_DATASET = True                              # shuffle the training data only\n",
    "RANDOM_SEED = 42                                    # randomly shuffle through indexed dataset\n",
    "\n",
    "# create indices for training and test split\n",
    "DATASET_SIZE = len(dataset)\n",
    "# list the dataset with an index for each entry\n",
    "indices = list(range(DATASET_SIZE))\n",
    "# define the split for the dataset\n",
    "split = int(np.floor(DATASET_SIZE * VALIDATION_SPLIT))\n",
    "if SHUFFLE_DATASET:\n",
    "    np.random.seed(RANDOM_SEED)\n",
    "    np.random.shuffle(indices)\n",
    "# split the dataset into training and testing \n",
    "train_indices, test_indices = indices[split:], indices[:split]\n",
    "\n",
    "# create data samplers and dataloaders\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "test_sampler = SubsetRandomSampler(test_indices)\n",
    "# create dataloaders\n",
    "trainloader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
    "testloader = DataLoader(dataset, batch_size=BATCH_SIZE, sampler=test_sampler)\n",
    "#print(len(trainloader), len(testloader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfda8d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = next(iter(trainloader))\n",
    "inputs_, labels_ = data['Inputs'], data['Labels']\n",
    "# print(inputs_.shape, labels_.shape) = torch.Size([16, 270, 256, 256]) torch.Size([16, 270, 256, 256])\n",
    "# crashes the kernel!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45469acb",
   "metadata": {},
   "source": [
    "The rest of the model...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789ca66f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
